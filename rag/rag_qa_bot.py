import os
import shutil
import sqlite3
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.chat_models import ChatOllama

# âœ… 1ï¸âƒ£ PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
file_path = "ì ˆëŒ€ê²½ë¡œ"  # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
## ëª¨ë¹Œë¦¬í‹° ê´€ë ¨ ëŒ€í•œë¯¼êµ­ ì •ë¶€ë¬¸ì„œë¥¼ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤.
if not os.path.exists(file_path):
    raise FileNotFoundError(f"ğŸš¨ PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")

# âœ… 2ï¸âƒ£ PDF ë¬¸ì„œ ë¡œë“œ
loader = PyPDFLoader(file_path)
pages = loader.load()
if not pages:
    raise ValueError("ğŸš¨ PDF ë¬¸ì„œì—ì„œ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤!")

# âœ… 3ï¸âƒ£ ë¬¸ì„œ ë¶„í•  (í…ìŠ¤íŠ¸ ì²­í¬ ë‹¨ìœ„)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, chunk_overlap=100
)
docs = text_splitter.split_documents(pages)

# âœ… 4ï¸âƒ£ ChromaDB ì €ì¥ ê²½ë¡œ ì„¤ì • ë° ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
chroma_persist_directory = os.path.abspath("./chroma_db")
shutil.rmtree(chroma_persist_directory, ignore_errors=True)  # ê¸°ì¡´ DB ì‚­ì œ
os.makedirs(chroma_persist_directory, exist_ok=True)  # ìƒˆ í´ë” ìƒì„±
os.chmod(chroma_persist_directory, 0o777)  # ê¶Œí•œ ì„¤ì •

# âœ… 5ï¸âƒ£ SQLite DB íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸ í›„ WAL ëª¨ë“œ í™œì„±í™”
db_path = os.path.join(chroma_persist_directory, "chroma.sqlite")
if os.path.exists(db_path):
    with sqlite3.connect(db_path) as conn:
        conn.execute("PRAGMA journal_mode=WAL;")

# âœ… 6ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# âœ… 7ï¸âƒ£ ChromaDB ë²¡í„° ì €ì¥ì†Œ ìƒì„± (`allow_reset` ì œê±°)
vectorstore = Chroma.from_documents(
    docs,
    embedding=embedding_model,
    persist_directory=chroma_persist_directory,
)

print("âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ! (ChromaDB ì‚¬ìš©)")

# âœ… 8ï¸âƒ£ ê²€ìƒ‰ ê¸°ëŠ¥ ì„¤ì • (ìœ ì‚¬ë„ ë†’ì€ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# âœ… 9ï¸âƒ£ Ollama ëª¨ë¸ ë¡œë“œ
llm = ChatOllama(model="gemma2:2b", temperature=0)

# âœ… ğŸ”Ÿ RAG í”„ë¡¬í”„íŠ¸ ì„¤ì •
template = '''Answer the question based only on the following context:
{context}

Question: {question}
'''
prompt = ChatPromptTemplate.from_template(template)

# âœ… 1ï¸âƒ£1ï¸âƒ£ ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
def format_docs(docs):
    return '\n\n'.join([d.page_content for d in docs])

# âœ… 1ï¸âƒ£2ï¸âƒ£ RAG Chain ì—°ê²°
rag_chain = (
    {'context': retriever | format_docs, 'question': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# âœ… 1ï¸âƒ£3ï¸âƒ£ ì§ˆë¬¸ ì‹¤í–‰
query = "ëª¨ë¹Œë¦¬í‹° ìë™ì°¨êµ­ ì±…ì„ìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?"
response = rag_chain.invoke(query)

# âœ… 1ï¸âƒ£4ï¸âƒ£ ê²°ê³¼ ì¶œë ¥
print("ğŸ”¹ ê²€ìƒ‰ëœ ë‹µë³€:")
print(response)
